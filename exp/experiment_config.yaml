# Unified experiment configuration file
# Each experiment script (00-sample, 01-dist, etc.) has its own section

# =============================================================================
# Global configuration (applies to all experiments)
# =============================================================================
global:
  # CUDA device configuration
  cuda:
    visible_devices: null  # null = use all available, or string like "0,1" or "0,1,2,3"

# =============================================================================
# 00-sample.py configuration
# =============================================================================
00_sample:
  # Task configuration: either specify task_config file OR configure directly
  # Option 1: Use a task config file (backward compatible)
  # task_config: "rl_onoff/tasks/configs/math_default.yaml"
  
  # Option 2: Configure task directly (more flexible)
  task:
    template_type: "chatml"  # Options: "openai", "llama", "chatml", "simple"
    
    reward_type: "math_verify"  # Only option available
    
    format_type: "boxed"  # Options: "boxed", "structured"

  dataset:
    name: "aime2025"  # Dataset name: "aime2025", "amc23", "gsm8k_level1", "math"
    split: "test"  # "test" or "train" (availability depends on dataset)
    num_examples: 10  # null = process all, or integer to limit

  backend:
    backend_type: "huggingface"
    model_name: "/data/chenyamei/pretrained_models/Qwen3-4B"
    backend_specific:
      device: null
      torch_dtype: null
      device_map: null

  # Sampling configuration: either specify sampling_config file OR configure directly
  # Option 1: Use a sampling config file (backward compatible)
  # sampling_config: "rl_onoff/sampling/configs/default.yaml"
  
  # Option 2: Configure sampling directly (more flexible)
  sampling:
    max_length: 2048  # Maximum generation length
    temperature: 1.0  # Sampling temperature (0.0 = deterministic)
    top_k: null  # Top-k sampling (null = disabled)
    top_p: null  # Nucleus sampling (null = disabled)
    do_sample: true  # Whether to use sampling (false = greedy)
    num_samples: 8  # Number of samples per prompt
    batch_size: 48  # Batch size for generation (null = no batching)
    seed: null  # Random seed (null = random)
    stop_strings: null #["\n\n"]  # Stop generation at these strings

  output:
    dir: "exp/00-sample/output"  # Output directory (relative to project root)


# =============================================================================
# 01-dist.py configuration
# =============================================================================
01_dist:
  input:
    results_file: "exp/00-sample/output/results.json"  # Path to results from 00-sample.py

  backend:
    backend_type: "huggingface"
    model_name: "/data/chenyamei/pretrained_models/Qwen3-4B"
    backend_specific:
      device: null
      torch_dtype: null
      device_map: null

  distribution:
    use_logits: false  # If true, extract logits; if false, extract probabilities
    temperature: 1.0  # Temperature for probability normalization
    top_k: 1000  # Number of top entries to store per token (reduces storage size)

  output:
    dir: "exp/01-dist/output"  # Output directory (relative to project root)


# =============================================================================
# 02-visualize.py configuration
# =============================================================================
02_visualize:
  input:
    results_dir: "exp/01-dist/output"  # Path to 01-dist output directory

  output:
    # Visualization doesn't save output, but you can add settings here if needed
    # e.g., default number of tokens to show, etc.


# =============================================================================
# 03-gradients.py configuration
# =============================================================================
03_gradients:
  input:
    results_file: "exp/00-sample/output/results.json"  # Path to results from 00-sample.py

  backend:
    backend_type: "huggingface"
    model_name: "/data/chenyamei/pretrained_models/Qwen3-4B"
    backend_specific:
      device: null
      torch_dtype: null
      device_map: null
      # LoRA configuration (required for gradient extraction)
      lora_config:
        r: 1  # LoRA rank
        lora_alpha: 32  # LoRA alpha scaling factor
        target_modules:  # Modules to apply LoRA to
          - "q_proj"
          - "k_proj"
          - "v_proj"
          - "o_proj"
          - "gate_proj"
          - "up_proj"
          - "down_proj"
        lora_dropout: 0.1
      # lora_adapter_path: null  # Optional: path to pre-trained LoRA adapter

  gradient:
    proj_dim: 8192  # Projection dimension (d)
    device: "cuda"  # Device for projection ("cuda" or "cpu")
    proj_type: "rademacher"  # Projection type: "normal" or "rademacher"
    use_cuda_projector: true  # Use CUDA projector (requires fast_jl)
    block_size: 100  # Block size for BasicProjector (if used)
    seed: 0  # Random seed for projector
    cuda_max_batch_size: 32  # Max batch size for CudaProjector (if used)
    token_chunk_size: 4096  # Maximum number of response tokens to process in a single gradient chunk.
    max_prompt_len: 1024  # Maximum total prompt length (prompt+response tokens) when computing gradients

  output:
    dir: "exp/03-gradients/output"  # Output directory (relative to project root)

